{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets torch evaluate seqeval scikit-learn accelerate indic-transliteration tqdm sentencepiece\n"
      ],
      "metadata": {
        "id": "rt3hlaCzuCjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6f14b1-2de4-4961-dda5-500dfecfdece"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Collecting indic-transliteration\n",
            "  Downloading indic_transliteration-2.3.75-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Collecting backports.functools-lru-cache (from indic-transliteration)\n",
            "  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (0.20.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (0.10.2)\n",
            "Collecting roman (from indic-transliteration)\n",
            "  Downloading roman-5.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading indic_transliteration-2.3.75-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\n",
            "Downloading roman-5.2-py3-none-any.whl (6.0 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=070d7d3afbc4f8521b8342aee85c0f9b844f0215e1e6ecd7fe15d06099b51dda\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: roman, backports.functools-lru-cache, seqeval, indic-transliteration, evaluate\n",
            "Successfully installed backports.functools-lru-cache-2.0.0 evaluate-0.4.6 indic-transliteration-2.3.75 roman-5.2 seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD6s-L1Urpfg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress potential warnings from the model loading process\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ----------------------------\n",
        "# Install transliteration lib if missing (runs silently in Colab/Jupyter)\n",
        "# ----------------------------\n",
        "try:\n",
        "    from indic_transliteration import sanscript\n",
        "    from indic_transliteration.sanscript import transliterate\n",
        "except Exception:\n",
        "    try:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"indic-transliteration\", \"-q\"])\n",
        "        from indic_transliteration import sanscript\n",
        "        from indic_transliteration.sanscript import transliterate\n",
        "    except Exception as e:\n",
        "        print(\"Warning: Could not install 'indic-transliteration'. Transliteration might not work.\")\n",
        "        sanscript = None\n",
        "        transliterate = None\n",
        "\n",
        "# ============================================================\n",
        "# PATH AND DATA CONFIGURATION\n",
        "# ============================================================\n",
        "data_dir = Path(\"cross_lingual_data\")  # <-- CHANGE THIS TO YOUR DATA FOLDER\n",
        "languages = ['as', 'bn', 'gu', 'ml', 'mr', 'ta', 'te']  # languages to load\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "output_dir = \"./indicbert-devanagari-ner-final\"\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"Data directory:\", data_dir.absolute())\n",
        "if not data_dir.exists():\n",
        "    print(\"FATAL ERROR: data directory not found. Exiting.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ----------------------------\n",
        "# SCRIPT MAP + Transliteration Helper\n",
        "# ----------------------------\n",
        "SCRIPT_MAP = {}\n",
        "if sanscript is not None:\n",
        "    SCRIPT_MAP = {\n",
        "        'as': sanscript.BENGALI,\n",
        "        'bn': sanscript.BENGALI,\n",
        "        'gu': sanscript.GUJARATI,\n",
        "        'ml': sanscript.MALAYALAM,\n",
        "        'mr': sanscript.DEVANAGARI,\n",
        "        'ta': sanscript.TAMIL,\n",
        "        'te': sanscript.TELUGU,\n",
        "    }\n",
        "\n",
        "def transliterate_to_devanagari(text, lang_code):\n",
        "    \"\"\"Transliterate text (a token) from source script to Devanagari.\"\"\"\n",
        "    if text is None:\n",
        "        return text\n",
        "    if lang_code == 'mr':\n",
        "        return text\n",
        "    if transliterate is None or lang_code not in SCRIPT_MAP:\n",
        "        return text\n",
        "    try:\n",
        "        # Use ITRANS mode to handle non-Indic foreign words better\n",
        "        return transliterate(text, SCRIPT_MAP[lang_code], sanscript.DEVANAGARI)\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "# ============================================================\n",
        "# 1. AUTHENTICATION & MODEL LOADING (Your requested block)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(\"STEP 1: Hugging Face Authentication\")\n",
        "print(\"---\")\n",
        "try:\n",
        "    # Use existing token or prompt for login\n",
        "    login(new_session=False)\n",
        "    print(\"✓ Authentication check passed (token found or successfully logged in).\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Warning: Authentication failed. Error: {e}\")\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(f\"STEP 2: Loading Base Model '{model_name}'\")\n",
        "print(\"---\")\n",
        "\n",
        "try:\n",
        "    # Load the Tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    print(\"✓ Tokenizer loaded.\")\n",
        "\n",
        "    # Model will be loaded again later as AutoModelForTokenClassification\n",
        "    # We do a basic load here to verify access\n",
        "    _ = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "    print(\"✓ Base Model access verified.\")\n",
        "    del _\n",
        "    gc.collect()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n=============================================================\")\n",
        "    print(f\"❌ FATAL ERROR: Could not load model '{model_name}'\")\n",
        "    print(\"=============================================================\")\n",
        "    print(f\"Details: {e}\")\n",
        "    print(\"\\nACTION REQUIRED: Ensure you accepted the license terms on the model page.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ============================================================\n",
        "# 3. LOAD, CONVERT, AND SPLIT DATA\n",
        "# ============================================================\n",
        "\n",
        "def load_and_convert_data(file_path: Path, lang_code: str):\n",
        "    data = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        total = sum(1 for line in f if line.strip())\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f, total=total, desc=f\"Loading {file_path.name}\", ncols=80):\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                words = item.get(\"words\") or item.get(\"tokens\") or []\n",
        "                ner = item.get(\"ner\") or item.get(\"ner_tags\") or []\n",
        "                if len(words) != len(ner): continue\n",
        "                tokens_dev = [transliterate_to_devanagari(w, lang_code) for w in words]\n",
        "                data.append({\"tokens\": tokens_dev, \"ner_tags\": ner})\n",
        "            except Exception: continue\n",
        "    return data\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Loading and Converting Data to Devanagari\")\n",
        "print(\"=\"*70)\n",
        "all_data = []\n",
        "for lang in languages:\n",
        "    file_path = data_dir / f\"{lang}_data.json\"\n",
        "    if file_path.exists():\n",
        "        lang_examples = load_and_convert_data(file_path, lang)\n",
        "        print(f\"  Loaded {len(lang_examples):,} examples for {lang}\")\n",
        "        all_data.extend(lang_examples)\n",
        "        del lang_examples; gc.collect()\n",
        "    else:\n",
        "        print(\"  NOT FOUND:\", file_path)\n",
        "\n",
        "if len(all_data) == 0: sys.exit(1)\n",
        "\n",
        "# Split data\n",
        "train_val, test = train_test_split(all_data, test_size=0.10, random_state=42)\n",
        "train, val = train_test_split(train_val, test_size=0.10, random_state=42)\n",
        "del all_data; gc.collect()\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train),\n",
        "    \"validation\": Dataset.from_list(val),\n",
        "    \"test\": Dataset.from_list(test)\n",
        "})\n",
        "\n",
        "# Labels\n",
        "all_labels = set()\n",
        "for item in train: all_labels.update(item[\"ner_tags\"])\n",
        "label_list = sorted(list(all_labels))\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "print(f\"\\nLabels: {label_list}\")\n",
        "print(f\"Total examples: {len(train):,} (Train), {len(val):,} (Val), {len(test):,} (Test)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. TOKENIZE AND ALIGN LABELS\n",
        "# ============================================================\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], is_split_into_words=True, truncation=True, padding=False, max_length=512\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label_seq[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100) # Subsequent sub-word token gets -100\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Tokenizing Dataset\")\n",
        "print(\"=\"*70)\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "# Load the specific model head for token classification\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 5. METRICS AND TRAINING\n",
        "# ============================================================\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "    for pred_seq, label_seq in zip(preds, labels):\n",
        "        pred_labels = []\n",
        "        true_label_list = []\n",
        "        for p, l in zip(pred_seq, label_seq):\n",
        "            if l != -100:\n",
        "                pred_labels.append(id2label[p])\n",
        "                true_label_list.append(id2label[l])\n",
        "        true_predictions.append(pred_labels)\n",
        "        true_labels.append(true_label_list)\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"]}\n",
        "\n",
        "Path(checkpoint_dir).mkdir(exist_ok=True)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint_dir, overwrite_output_dir=True, save_strategy=\"steps\", save_steps=1000,\n",
        "    save_total_limit=3, eval_strategy=\"steps\", eval_steps=1000, load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\", greater_is_better=True, learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16, per_device_eval_batch_size=32, num_train_epochs=5,\n",
        "    weight_decay=0.01, gradient_accumulation_steps=2, fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=4, logging_dir=\"./logs\", logging_steps=100, logging_strategy=\"steps\",\n",
        "    seed=42, push_to_hub=False, report_to=\"none\"\n",
        ")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"], tokenizer=tokenizer,\n",
        "    data_collator=data_collator, compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Training (with auto-resume if checkpoints exist)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: Training IndicBERT\")\n",
        "print(\"=\"*70)\n",
        "existing_checkpoints = list(Path(checkpoint_dir).glob(\"checkpoint-*\"))\n",
        "if existing_checkpoints:\n",
        "    print(f\"Found {len(existing_checkpoints)} checkpoint(s). Resuming training.\")\n",
        "\n",
        "try:\n",
        "    trainer.train(resume_from_checkpoint=True if existing_checkpoints else None)\n",
        "    print(\"\\nTraining completed.\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user. Checkpoints saved.\")\n",
        "except Exception as e:\n",
        "    print(\"\\nTraining failed with exception:\", e)\n",
        "    raise\n",
        "\n",
        "# ============================================================\n",
        "# 6. SAVE FINAL MODEL AND EVALUATE\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: Saving and Evaluating\")\n",
        "print(\"=\"*70)\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nFinal model saved to: {output_dir}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
        "print(\"\\nTEST RESULTS\")\n",
        "print(f\"Precision: {test_results.get('eval_precision', 0.0):.4f}\")\n",
        "print(f\"Recall:    {test_results.get('eval_recall', 0.0):.4f}\")\n",
        "print(f\"F1 Score:  {test_results.get('eval_f1', 0.0):.4f}\")\n",
        "\n",
        "print(\"\\nDONE.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PV7Z4EV2BI9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}